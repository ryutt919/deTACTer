{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# VAEP 모델 비교: CatBoost vs XGBoost\n",
                "\n",
                "이 노트북에서는 K-League 데이터를 사용하여 VAEP 점수를 계산하기 위한 두 가지 강력한 부스팅 모델인 CatBoost와 XGBoost의 성능을 비교합니다.\n",
                "\n",
                "## 1. 환경 설정 및 데이터 로드"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import yaml\n",
                "import os\n",
                "import glob\n",
                "import numpy as np\n",
                "from sklearn.metrics import roc_auc_score, roc_curve\n",
                "from sklearn.model_selection import train_test_split\n",
                "from catboost import CatBoostClassifier\n",
                "from xgboost import XGBClassifier\n",
                "import matplotlib.pyplot as plt\n",
                "import socceraction.vaep.features as features\n",
                "import socceraction.vaep.labels as labels\n",
                "\n",
                "# 설정 로드\n",
                "with open('../config.yaml', 'r') as f:\n",
                "    config = yaml.safe_load(f)\n",
                "\n",
                "SPADL_DIR = os.path.join('..', config['data']['spadl_output_dir'])\n",
                "NB_PREV_ACTIONS = config['features']['nb_prev_actions']\n",
                "\n",
                "def load_spadl_data():\n",
                "    all_games = []\n",
                "    files = glob.glob(os.path.join(SPADL_DIR, \"game_*.csv\"))\n",
                "    print(f\"Loading {len(files)} games for model comparison...\")\n",
                "    for f in files:\n",
                "        df = pd.read_csv(f)\n",
                "        all_games.append(df)\n",
                "    return all_games\n",
                "\n",
                "games = load_spadl_data()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Feature 및 Label 생성"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_feature_functions():\n",
                "    return [\n",
                "        features.actiontype_onehot,\n",
                "        features.result_onehot,\n",
                "        features.actiontype_result_onehot,\n",
                "        features.bodypart_onehot,\n",
                "        features.time,\n",
                "        features.startlocation,\n",
                "        features.endlocation,\n",
                "        features.movement,\n",
                "        features.space_delta,\n",
                "        features.team,\n",
                "    ]\n",
                "\n",
                "def generate_dataset(games):\n",
                "    X_list = []\n",
                "    Y_list = []\n",
                "    xfns = get_feature_functions()\n",
                "\n",
                "    print(\"Generating features and labels...\")\n",
                "    for game in games:\n",
                "        try:\n",
                "            gamestates = features.gamestates(game, NB_PREV_ACTIONS)\n",
                "            X_game = pd.concat([fn(gamestates) for fn in xfns], axis=1)\n",
                "            \n",
                "            Y_scores = labels.scores(game, nr_actions=10)\n",
                "            Y_concedes = labels.concedes(game, nr_actions=10)\n",
                "            Y_game = pd.concat([Y_scores, Y_concedes], axis=1)\n",
                "            Y_game.columns = ['scores', 'concedes']\n",
                "            \n",
                "            X_list.append(X_game)\n",
                "            Y_list.append(Y_game)\n",
                "        except Exception as e:\n",
                "            print(f\"Skipping game due to error: {e}\")\n",
                "            continue\n",
                "\n",
                "    if not X_list:\n",
                "        raise ValueError(\"No data generated.\")\n",
                "\n",
                "    X = pd.concat(X_list).reset_index(drop=True)\n",
                "    Y = pd.concat(Y_list).reset_index(drop=True)\n",
                "    return X, Y\n",
                "\n",
                "X, Y = generate_dataset(games)\n",
                "print(f\"Dataset shape: {X.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. 모델 학습 및 평가\n",
                "\n",
                "데이터를 학습 세트와 검증 세트로 나누고(80:20), 각 모델을 학습시킵니다.\n",
                "- **CatBoost**: 범주형 변수 처리에 강점이 있으며 기본 설정으로도 좋은 성능을 보입니다.\n",
                "- **XGBoost**: 빠르고 효율적이며 튜닝 시 높은 성능을 낼 수 있습니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
                "\n",
                "models = {\n",
                "    'CatBoost': CatBoostClassifier(verbose=0, random_state=42),\n",
                "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
                "}\n",
                "\n",
                "results = {}\n",
                "\n",
                "for name, model in models.items():\n",
                "    print(f\"Training {name}...\")\n",
                "    # Scoring Model\n",
                "    model.fit(X_train, Y_train['scores'])\n",
                "    pred_scores = model.predict_proba(X_val)[:, 1]\n",
                "    auc_scores = roc_auc_score(Y_val['scores'], pred_scores)\n",
                "    \n",
                "    # Conceding Model\n",
                "    model.fit(X_train, Y_train['concedes'])\n",
                "    pred_concedes = model.predict_proba(X_val)[:, 1]\n",
                "    auc_concedes = roc_auc_score(Y_val['concedes'], pred_concedes)\n",
                "    \n",
                "    results[name] = {\n",
                "        'AUC (Scores)': auc_scores,\n",
                "        'AUC (Concedes)': auc_concedes,\n",
                "        'Average AUC': (auc_scores + auc_concedes) / 2,\n",
                "        'pred_scores': pred_scores,\n",
                "        'pred_concedes': pred_concedes\n",
                "    }\n",
                "    print(f\"{name} Results: {results[name]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. 결과 시각화 (ROC Curve)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "# Scoring ROC\n",
                "plt.subplot(1, 2, 1)\n",
                "for name in models.keys():\n",
                "    fpr, tpr, _ = roc_curve(Y_val['scores'], results[name]['pred_scores'])\n",
                "    plt.plot(fpr, tpr, label=f\"{name} (AUC={results[name]['AUC (Scores)']:.3f})\")\n",
                "plt.plot([0, 1], [0, 1], 'k--')\n",
                "plt.title('ROC Curve - Scoring')\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.legend()\n",
                "\n",
                "# Conceding ROC\n",
                "plt.subplot(1, 2, 2)\n",
                "for name in models.keys():\n",
                "    fpr, tpr, _ = roc_curve(Y_val['concedes'], results[name]['pred_concedes'])\n",
                "    plt.plot(fpr, tpr, label=f\"{name} (AUC={results[name]['AUC (Concedes)']:.3f})\")\n",
                "plt.plot([0, 1], [0, 1], 'k--')\n",
                "plt.title('ROC Curve - Conceding')\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../results/model_comparison_roc.png')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. 결론\n",
                "\n",
                "위의 ROC 커브와 AUC 점수를 통해 어떤 모델이 K-League 데이터셋에 더 적합한지 판단할 수 있습니다. 일반적으로 두 모델 모두 우수한 성능을 보이지만, 데이터 특성에 따라 미세한 차이가 발생할 수 있습니다."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}